@article{opperStartingSmallAll2024,
  title = {Starting {{Small}}, {{After All}}? {{Curriculum Learning}} with {{Child-Directed Speech}}},
  shorttitle = {Starting {{Small}}, {{After All}}?},
  author = {Opper, Mattia and family=Souza, given=Sydelle, prefix=de, useprefix=true},
  date = {2024},
  journaltitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {46},
  number = {0},
  urldate = {2024-10-03},
  abstract = {The idea of curriculum learning, whereby a model is first exposed to simpler examples before an increase in complexity, has long fascinated the AI community. Unfortunately, the experimental successes of curriculum learning have been mixed, particularly applied to natural language, where a vast body of literature appears to evidence its failures. However, recent work has shown that language models trained on transcribed-child-directed-speech (CDS) learn more grammar compared to those trained on Wikipedia. To a lesser extent, the same trend has been observed through training on transcribed speech and simple text data. Motivated by these findings, we revisit the idea of curriculum learning starting from CDS, before moving to simple data, and finally finishing with complex long form text. Unfortunately, through experimentation with an array of models and training step sizes, only in the smallest models trained for the least steps does curriculum learning show any advantage over random sampling.},
  langid = {english},
  file = {C:\Users\psyde\Zotero\storage\3XJIVKRX\Opper and de Souza - 2024 - Starting Small, After All Curriculum Learning wit.pdf}
}

